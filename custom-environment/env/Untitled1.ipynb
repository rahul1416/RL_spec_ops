{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c1d8001-0dbd-43dd-b135-967a4ddb7127",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-19 14:23:36,694\tWARNING __init__.py:10 -- PG has/have been moved to `rllib_contrib` and will no longer be maintained by the RLlib team. You can still use it/them normally inside RLlib util Ray 2.8, but from Ray 2.9 on, all `rllib_contrib` algorithms will no longer be part of the core repo, and will therefore have to be installed separately with pinned dependencies for e.g. ray[rllib] and other packages! See https://github.com/ray-project/ray/tree/master/rllib_contrib#rllib-contrib for more information on the RLlib contrib effort.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import ray\n",
    "import supersuit as ss\n",
    "from ray import tune\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.rllib.algorithms.ppo import PPO\n",
    "from ray.rllib.env.wrappers.pettingzoo_env import ParallelPettingZooEnv\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.tune.registry import register_env\n",
    "from torch import nn\n",
    "from custom_environment import Spec_Ops_Env\n",
    "    \n",
    "def env_creator():\n",
    "    env = Spec_Ops_Env()    \n",
    "    return env\n",
    "\n",
    "def policy_map_fn(agent_id: str, _episode=None, _worker=None, **_kwargs) -> str:\n",
    "    return agent_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cc6f07c-4c0d-4a87-bbfc-c6047eb29162",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-19 14:23:40,486\tINFO worker.py:1673 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "NEE YABBA TORCH CUDA UNDA?: True \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ray.init(ignore_reinit_error=True, num_gpus=1)\n",
    "\n",
    "env_name = \"123\"\n",
    "\n",
    "print(\"\\n\\n\\nNEE YABBA TORCH CUDA UNDA?:\", torch.cuda.is_available(),'\\n\\n\\n')\n",
    "register_env(env_name, lambda config: ParallelPettingZooEnv(env_creator()))\n",
    "\n",
    "env=env_creator()\n",
    "\n",
    "config = (\n",
    "        PPOConfig()\n",
    "        .environment(env=\"123\", clip_actions=True)\n",
    "        .rollouts(num_rollout_workers=4, rollout_fragment_length=128)\n",
    "        .training(\n",
    "            train_batch_size=512,\n",
    "            lr=2e-5,\n",
    "            gamma=0.99,\n",
    "            lambda_=0.9,\n",
    "            use_gae=True,\n",
    "            clip_param=0.4,\n",
    "            grad_clip=None,\n",
    "            entropy_coeff=0.1,\n",
    "            vf_loss_coeff=0.25,\n",
    "            sgd_minibatch_size=64,\n",
    "            num_sgd_iter=10,\n",
    "        )\n",
    "        .multi_agent(\n",
    "            policies=env.possible_agents,\n",
    "            policy_mapping_fn=(lambda agent_id, *args, **kwargs: agent_id),\n",
    "        )\n",
    "        .debugging(log_level=\"ERROR\")\n",
    "        .framework(framework=\"torch\")\n",
    "        .resources(num_gpus=1)#int(os.environ.get(\"RLLIB_NUM_GPUS\", \"0\"))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b494f4d8-38c9-4999-bb32-eb4eebdad361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'soldier_0': PPOTorchPolicy, 'terrorist_0': PPOTorchPolicy}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from ray.rllib.policy.policy import Policy\n",
    "\n",
    "# Use the `from_checkpoint` utility of the Policy class:\n",
    "my_restored_policy = Policy.from_checkpoint('/home/hemanthgaddey/RL_Spec_Ops_logs/PPO/PPO_123_a44ad_00000_0_2023-11-19_04-54-38/checkpoint_000006')\n",
    "print(my_restored_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb9f1018-84cd-4921-94cc-771ac8f18420",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hemanthgaddey/Documents/RL_Spec_Ops/venv/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:484: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/home/hemanthgaddey/Documents/RL_Spec_Ops/venv/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/hemanthgaddey/Documents/RL_Spec_Ops/venv/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/hemanthgaddey/Documents/RL_Spec_Ops/venv/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "\u001b[36m(RolloutWorker pid=28163)\u001b[0m 2023-11-19 13:02:54,948\tWARNING __init__.py:10 -- PG has/have been moved to `rllib_contrib` and will no longer be maintained by the RLlib team. You can still use it/them normally inside RLlib util Ray 2.8, but from Ray 2.9 on, all `rllib_contrib` algorithms will no longer be part of the core repo, and will therefore have to be installed separately with pinned dependencies for e.g. ray[rllib] and other packages! See https://github.com/ray-project/ray/tree/master/rllib_contrib#rllib-contrib for more information on the RLlib contrib effort.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RolloutWorker pid=28163)\u001b[0m No Seeding only CHAOS!!!!!\n",
      "\u001b[36m(RolloutWorker pid=28163)\u001b[0m \n",
      "\u001b[36m(RolloutWorker pid=28163)\u001b[0m \n",
      "\u001b[36m(RolloutWorker pid=28163)\u001b[0m \n",
      "\u001b[36m(RolloutWorker pid=28163)\u001b[0m AGENT COORDINATES: 27 50 terrorist_0 \n",
      "\u001b[36m(RolloutWorker pid=28163)\u001b[0m \n",
      "\u001b[36m(RolloutWorker pid=28163)\u001b[0m \n",
      "\u001b[36m(RolloutWorker pid=28163)\u001b[0m \n",
      "\u001b[36m(RolloutWorker pid=28163)\u001b[0m \n",
      "\u001b[36m(RolloutWorker pid=28163)\u001b[0m \n",
      "\u001b[36m(RolloutWorker pid=28163)\u001b[0m \n",
      "\u001b[36m(RolloutWorker pid=28163)\u001b[0m AGENT COORDINATES: 71 28 soldier_0 \n",
      "\u001b[36m(RolloutWorker pid=28163)\u001b[0m \n",
      "\u001b[36m(RolloutWorker pid=28163)\u001b[0m \n",
      "\u001b[36m(RolloutWorker pid=28163)\u001b[0m \n",
      "\u001b[36m(RolloutWorker pid=28163)\u001b[0m \n",
      "\u001b[36m(RolloutWorker pid=28163)\u001b[0m \n",
      "\u001b[36m(RolloutWorker pid=28163)\u001b[0m \n",
      "\u001b[36m(RolloutWorker pid=28163)\u001b[0m AGENT COORDINATES: 71 60 terrorist_0 \n",
      "\u001b[36m(RolloutWorker pid=28163)\u001b[0m \n",
      "\u001b[36m(RolloutWorker pid=28163)\u001b[0m \n",
      "\u001b[36m(RolloutWorker pid=28163)\u001b[0m \n",
      "\u001b[36m(RolloutWorker pid=28163)\u001b[0m \n",
      "\u001b[36m(RolloutWorker pid=28163)\u001b[0m \n",
      "\u001b[36m(RolloutWorker pid=28163)\u001b[0m \n",
      "\u001b[36m(RolloutWorker pid=28163)\u001b[0m AGENT COORDINATES: 36 50 soldier_0 \n",
      "\u001b[36m(RolloutWorker pid=28163)\u001b[0m \n",
      "\u001b[36m(RolloutWorker pid=28163)\u001b[0m \n",
      "\u001b[36m(RolloutWorker pid=28163)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RolloutWorker pid=28451)\u001b[0m 2023-11-19 13:03:02,857\tWARNING __init__.py:10 -- PG has/have been moved to `rllib_contrib` and will no longer be maintained by the RLlib team. You can still use it/them normally inside RLlib util Ray 2.8, but from Ray 2.9 on, all `rllib_contrib` algorithms will no longer be part of the core repo, and will therefore have to be installed separately with pinned dependencies for e.g. ray[rllib] and other packages! See https://github.com/ray-project/ray/tree/master/rllib_contrib#rllib-contrib for more information on the RLlib contrib effort.\u001b[32m [repeated 5x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RolloutWorker pid=28451)\u001b[0m No Seeding only CHAOS!!!!!\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(RolloutWorker pid=28451)\u001b[0m \u001b[32m [repeated 108x across cluster]\u001b[0m\n",
      "\u001b[36m(RolloutWorker pid=28451)\u001b[0m AGENT COORDINATES: 51 12 soldier_0 \u001b[32m [repeated 18x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-19 13:03:05,035\tINFO trainable.py:164 -- Trainable.setup took 13.084 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2023-11-19 13:03:05,041\tWARNING util.py:62 -- Install gputil for GPU system monitoring.\n",
      "2023-11-19 13:03:06,908\tERROR actor_manager.py:500 -- Ray error, taking actor 2 out of service. The actor died unexpectedly before finishing this task.\n",
      "2023-11-19 13:03:06,908\tERROR actor_manager.py:500 -- Ray error, taking actor 4 out of service. The actor died unexpectedly before finishing this task.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO PPOTorchPolicy PPOTorchPolicy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m [2023-11-19 13:03:32,861 E 25137 25137] (raylet) node_manager.cc:3035: 67 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: b41888a6db45594aaebfc5fb61266ee8747542a214edb63a4172d115, IP: 10.10.18.9) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.10.18.9`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.algorithm import Algorithm\n",
    "\n",
    "loaded_ppo = Algorithm.from_checkpoint('/home/hemanthgaddey/RL_Spec_Ops_logs/PPO/PPO_123_a44ad_00000_0_2023-11-19_04-54-38/checkpoint_000006')\n",
    "\n",
    "\n",
    "loaded_policy_terr = loaded_ppo.get_policy('terrorist_0')\n",
    "loaded_policy_sol = loaded_ppo.get_policy('soldier_0')\n",
    "print(loaded_ppo, loaded_policy_terr, loaded_policy_sol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "791c2349-44b6-4223-8beb-b132ebf3de2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-19 13:11:09,854\tERROR checker.py:258 -- Exception mat1 and mat2 shapes cannot be multiplied (1x6400 and 64000x256) raised on function call without checkin input specs. RLlib will now attempt to check the spec before calling the function again.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Seeding only CHAOS!!!!!\n",
      "\n",
      "\n",
      "\n",
      "AGENT COORDINATES: 7 28 terrorist_0 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "AGENT COORDINATES: 71 60 soldier_0 \n",
      "\n",
      "\n",
      "\n",
      "[3 3 3 ... 3 3 3]\n"
     ]
    },
    {
     "ename": "SpecCheckingError",
     "evalue": "input spec validation failed on TorchMLPEncoder.forward, Mismatch found in data element ('obs',), which is a TensorSpec: Expected shape ('b', 64000) but found (1, 6400).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/RL_Spec_Ops/venv/lib/python3.11/site-packages/ray/rllib/core/models/specs/specs_dict.py:130\u001b[0m, in \u001b[0;36mSpecDict.validate\u001b[0;34m(self, data, exact_match)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 130\u001b[0m     \u001b[43mspec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_to_validate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Documents/RL_Spec_Ops/venv/lib/python3.11/site-packages/ray/rllib/core/models/specs/specs_base.py:243\u001b[0m, in \u001b[0;36mTensorSpec.validate\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(expected_d, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m expected_d \u001b[38;5;241m!=\u001b[39m actual_d:\n\u001b[0;32m--> 243\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(_INVALID_SHAPE\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expected_shape, shape))\n\u001b[1;32m    245\u001b[0m dtype \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39mdtype\n",
      "\u001b[0;31mValueError\u001b[0m: Expected shape ('b', 64000) but found (1, 6400)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/RL_Spec_Ops/venv/lib/python3.11/site-packages/ray/rllib/core/models/specs/checker.py:160\u001b[0m, in \u001b[0;36m_validate\u001b[0;34m(cls_instance, method, data, spec, filter, tag)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     \u001b[43mspec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Documents/RL_Spec_Ops/venv/lib/python3.11/site-packages/ray/rllib/core/models/specs/specs_dict.py:132\u001b[0m, in \u001b[0;36mSpecDict.validate\u001b[0;34m(self, data, exact_match)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 132\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    133\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMismatch found in data element \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    134\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhich is a TensorSpec: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    135\u001b[0m         )\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(spec, (\u001b[38;5;28mtype\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n",
      "\u001b[0;31mValueError\u001b[0m: Mismatch found in data element ('obs',), which is a TensorSpec: Expected shape ('b', 64000) but found (1, 6400)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mSpecCheckingError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m----> 6\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43mmy_restored_policy\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msoldier_0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_single_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msoldier_0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     obs, reward, done, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# Do something with the obtained results\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# For example, print the action taken and received reward\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/RL_Spec_Ops/venv/lib/python3.11/site-packages/ray/rllib/policy/policy.py:558\u001b[0m, in \u001b[0;36mPolicy.compute_single_action\u001b[0;34m(self, obs, state, prev_action, prev_reward, info, input_dict, episode, explore, timestep, **kwargs)\u001b[0m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m episode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    556\u001b[0m     episodes \u001b[38;5;241m=\u001b[39m [episode]\n\u001b[0;32m--> 558\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_actions_from_input_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSampleBatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dict\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepisodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexplore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexplore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimestep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimestep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;66;03m# Some policies don't return a tuple, but always just a single action.\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;66;03m# E.g. ES and ARS.\u001b[39;00m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mtuple\u001b[39m):\n",
      "File \u001b[0;32m~/Documents/RL_Spec_Ops/venv/lib/python3.11/site-packages/ray/rllib/policy/torch_policy_v2.py:551\u001b[0m, in \u001b[0;36mTorchPolicyV2.compute_actions_from_input_dict\u001b[0;34m(self, input_dict, explore, timestep, **kwargs)\u001b[0m\n\u001b[1;32m    549\u001b[0m input_dict\u001b[38;5;241m.\u001b[39mset_training(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_enable_rl_module_api\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_action_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_batches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseq_lens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexplore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexplore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimestep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimestep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    559\u001b[0m     \u001b[38;5;66;03m# Pack internal state inputs into (separate) list.\u001b[39;00m\n\u001b[1;32m    560\u001b[0m     state_batches \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    561\u001b[0m         input_dict[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m input_dict\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate_in\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m k[:\u001b[38;5;241m8\u001b[39m]\n\u001b[1;32m    562\u001b[0m     ]\n",
      "File \u001b[0;32m~/Documents/RL_Spec_Ops/venv/lib/python3.11/site-packages/ray/rllib/utils/threading.py:24\u001b[0m, in \u001b[0;36mwith_lock.<locals>.wrapper\u001b[0;34m(self, *a, **k)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m---> 24\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_lock\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m e\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[0;32m~/Documents/RL_Spec_Ops/venv/lib/python3.11/site-packages/ray/rllib/policy/torch_policy_v2.py:1207\u001b[0m, in \u001b[0;36mTorchPolicyV2._compute_action_helper\u001b[0;34m(self, input_dict, state_batches, seq_lens, explore, timestep)\u001b[0m\n\u001b[1;32m   1204\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m input_dict[SampleBatch\u001b[38;5;241m.\u001b[39mSEQ_LENS]\n\u001b[1;32m   1206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m explore:\n\u001b[0;32m-> 1207\u001b[0m     fwd_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_exploration\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1208\u001b[0m     \u001b[38;5;66;03m# For recurrent models, we need to remove the time dimension.\u001b[39;00m\n\u001b[1;32m   1209\u001b[0m     fwd_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaybe_remove_time_dimension(fwd_out)\n",
      "File \u001b[0;32m~/Documents/RL_Spec_Ops/venv/lib/python3.11/site-packages/ray/rllib/core/models/specs/checker.py:253\u001b[0m, in \u001b[0;36mcheck_input_specs.<locals>.decorator.<locals>.wrapper\u001b[0;34m(self, input_data, **kwargs)\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, input_data, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SpecCheckingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 253\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;66;03m# We store the initial exception to raise it later if the spec\u001b[39;00m\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;66;03m# check fails.\u001b[39;00m\n\u001b[1;32m    257\u001b[0m     initial_exception \u001b[38;5;241m=\u001b[39m e\n",
      "File \u001b[0;32m~/Documents/RL_Spec_Ops/venv/lib/python3.11/site-packages/ray/rllib/core/models/specs/checker.py:251\u001b[0m, in \u001b[0;36mcheck_input_specs.<locals>.decorator.<locals>.wrapper\u001b[0;34m(self, input_data, **kwargs)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m only_check_on_retry:\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;66;03m# Attempt to run the function without spec checking\u001b[39;00m\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 251\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m SpecCheckingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    253\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/Documents/RL_Spec_Ops/venv/lib/python3.11/site-packages/ray/rllib/core/models/specs/checker.py:361\u001b[0m, in \u001b[0;36mcheck_output_specs.<locals>.decorator.<locals>.wrapper\u001b[0;34m(self, input_data, **kwargs)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__checked_output_specs_cache__\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__checked_output_specs_cache__ \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 361\u001b[0m output_data \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_specs:\n\u001b[1;32m    364\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, output_specs):\n",
      "File \u001b[0;32m~/Documents/RL_Spec_Ops/venv/lib/python3.11/site-packages/ray/rllib/core/rl_module/rl_module.py:633\u001b[0m, in \u001b[0;36mRLModule.forward_exploration\u001b[0;34m(self, batch, **kwargs)\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;129m@check_input_specs\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_input_specs_exploration\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    615\u001b[0m \u001b[38;5;129m@check_output_specs\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_output_specs_exploration\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_exploration\u001b[39m(\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28mself\u001b[39m, batch: SampleBatchType, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    618\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Mapping[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m    619\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Forward-pass during exploration, called from the sampler.\u001b[39;00m\n\u001b[1;32m    620\u001b[0m \n\u001b[1;32m    621\u001b[0m \u001b[38;5;124;03m    This method should not be overriden to implement a custom forward exploration\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;124;03m        ouptut_specs_exploration().\u001b[39;00m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_exploration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/RL_Spec_Ops/venv/lib/python3.11/site-packages/ray/rllib/algorithms/ppo/torch/ppo_torch_rl_module.py:43\u001b[0m, in \u001b[0;36mPPOTorchRLModule._forward_exploration\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     40\u001b[0m output \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Shared encoder\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m encoder_outs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m STATE_OUT \u001b[38;5;129;01min\u001b[39;00m encoder_outs:\n\u001b[1;32m     45\u001b[0m     output[STATE_OUT] \u001b[38;5;241m=\u001b[39m encoder_outs[STATE_OUT]\n",
      "File \u001b[0;32m~/Documents/RL_Spec_Ops/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/RL_Spec_Ops/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/RL_Spec_Ops/venv/lib/python3.11/site-packages/ray/rllib/core/models/specs/checker.py:253\u001b[0m, in \u001b[0;36mcheck_input_specs.<locals>.decorator.<locals>.wrapper\u001b[0;34m(self, input_data, **kwargs)\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, input_data, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SpecCheckingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 253\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;66;03m# We store the initial exception to raise it later if the spec\u001b[39;00m\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;66;03m# check fails.\u001b[39;00m\n\u001b[1;32m    257\u001b[0m     initial_exception \u001b[38;5;241m=\u001b[39m e\n",
      "File \u001b[0;32m~/Documents/RL_Spec_Ops/venv/lib/python3.11/site-packages/ray/rllib/core/models/specs/checker.py:251\u001b[0m, in \u001b[0;36mcheck_input_specs.<locals>.decorator.<locals>.wrapper\u001b[0;34m(self, input_data, **kwargs)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m only_check_on_retry:\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;66;03m# Attempt to run the function without spec checking\u001b[39;00m\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 251\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m SpecCheckingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    253\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/Documents/RL_Spec_Ops/venv/lib/python3.11/site-packages/ray/rllib/core/models/torch/base.py:120\u001b[0m, in \u001b[0;36mTorchModel.forward\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward(input_data, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m checked_forward(\u001b[38;5;28mself\u001b[39m, inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/RL_Spec_Ops/venv/lib/python3.11/site-packages/ray/rllib/core/models/base.py:343\u001b[0m, in \u001b[0;36mActorCriticEncoder._forward\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    336\u001b[0m         ENCODER_OUT: {\n\u001b[1;32m    337\u001b[0m             ACTOR: encoder_outs[ENCODER_OUT],\n\u001b[1;32m    338\u001b[0m             CRITIC: encoder_outs[ENCODER_OUT],\n\u001b[1;32m    339\u001b[0m         }\n\u001b[1;32m    340\u001b[0m     }\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;66;03m# Encoders should not modify inputs, so we can pass the same inputs\u001b[39;00m\n\u001b[0;32m--> 343\u001b[0m     actor_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    344\u001b[0m     critic_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic_encoder(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    347\u001b[0m         ENCODER_OUT: {\n\u001b[1;32m    348\u001b[0m             ACTOR: actor_out[ENCODER_OUT],\n\u001b[1;32m    349\u001b[0m             CRITIC: critic_out[ENCODER_OUT],\n\u001b[1;32m    350\u001b[0m         }\n\u001b[1;32m    351\u001b[0m     }\n",
      "File \u001b[0;32m~/Documents/RL_Spec_Ops/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/RL_Spec_Ops/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/RL_Spec_Ops/venv/lib/python3.11/site-packages/ray/rllib/core/models/specs/checker.py:276\u001b[0m, in \u001b[0;36mcheck_input_specs.<locals>.decorator.<locals>.wrapper\u001b[0;34m(self, input_data, **kwargs)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    275\u001b[0m     spec \u001b[38;5;241m=\u001b[39m convert_to_canonical_format(spec)\n\u001b[0;32m--> 276\u001b[0m     checked_data \u001b[38;5;241m=\u001b[39m \u001b[43m_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcls_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mspec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mfilter\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(checked_data, NestedDict):\n\u001b[1;32m    286\u001b[0m         \u001b[38;5;66;03m# filtering should happen regardless of cache\u001b[39;00m\n\u001b[1;32m    287\u001b[0m         checked_data \u001b[38;5;241m=\u001b[39m checked_data\u001b[38;5;241m.\u001b[39mfilter(spec)\n",
      "File \u001b[0;32m~/Documents/RL_Spec_Ops/venv/lib/python3.11/site-packages/ray/rllib/core/models/specs/checker.py:162\u001b[0m, in \u001b[0;36m_validate\u001b[0;34m(cls_instance, method, data, spec, filter, tag)\u001b[0m\n\u001b[1;32m    160\u001b[0m         spec\u001b[38;5;241m.\u001b[39mvalidate(data)\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 162\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SpecCheckingError(\n\u001b[1;32m    163\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m spec validation failed on \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    164\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcls_instance\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    165\u001b[0m         )\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[0;31mSpecCheckingError\u001b[0m: input spec validation failed on TorchMLPEncoder.forward, Mismatch found in data element ('obs',), which is a TensorSpec: Expected shape ('b', 64000) but found (1, 6400)."
     ]
    }
   ],
   "source": [
    "\n",
    "obs = env.reset()\n",
    "print(obs[0]['soldier_0'])\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = my_restored_policy.compute(obs[0])\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    \n",
    "    # Do something with the obtained results\n",
    "    # For example, print the action taken and received reward\n",
    "    print(f\"Action taken: {action}, Received reward: {reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "856c7b1f-9119-4f8e-aee2-88bb75b077c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hemanthgaddey/Documents/RL_Spec_Ops/venv/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:484: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/home/hemanthgaddey/Documents/RL_Spec_Ops/venv/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/hemanthgaddey/Documents/RL_Spec_Ops/venv/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/hemanthgaddey/Documents/RL_Spec_Ops/venv/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "\u001b[36m(RolloutWorker pid=8607)\u001b[0m 2023-11-19 14:23:50,549\tWARNING __init__.py:10 -- PG has/have been moved to `rllib_contrib` and will no longer be maintained by the RLlib team. You can still use it/them normally inside RLlib util Ray 2.8, but from Ray 2.9 on, all `rllib_contrib` algorithms will no longer be part of the core repo, and will therefore have to be installed separately with pinned dependencies for e.g. ray[rllib] and other packages! See https://github.com/ray-project/ray/tree/master/rllib_contrib#rllib-contrib for more information on the RLlib contrib effort.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RolloutWorker pid=8607)\u001b[0m No Seeding only CHAOS!!!!!\n",
      "\u001b[36m(RolloutWorker pid=8607)\u001b[0m \n",
      "\u001b[36m(RolloutWorker pid=8607)\u001b[0m \n",
      "\u001b[36m(RolloutWorker pid=8607)\u001b[0m \n",
      "\u001b[36m(RolloutWorker pid=8607)\u001b[0m AGENT COORDINATES: 76 28 terrorist_0 \n",
      "\u001b[36m(RolloutWorker pid=8607)\u001b[0m \n",
      "\u001b[36m(RolloutWorker pid=8607)\u001b[0m \n",
      "\u001b[36m(RolloutWorker pid=8607)\u001b[0m \n",
      "\u001b[36m(RolloutWorker pid=8607)\u001b[0m \n",
      "\u001b[36m(RolloutWorker pid=8607)\u001b[0m \n",
      "\u001b[36m(RolloutWorker pid=8607)\u001b[0m \n",
      "\u001b[36m(RolloutWorker pid=8607)\u001b[0m AGENT COORDINATES: 41 12 soldier_0 \n",
      "\u001b[36m(RolloutWorker pid=8607)\u001b[0m \n",
      "\u001b[36m(RolloutWorker pid=8607)\u001b[0m \n",
      "\u001b[36m(RolloutWorker pid=8607)\u001b[0m \n",
      "\u001b[36m(RolloutWorker pid=8607)\u001b[0m \n",
      "\u001b[36m(RolloutWorker pid=8607)\u001b[0m \n",
      "\u001b[36m(RolloutWorker pid=8607)\u001b[0m \n",
      "\u001b[36m(RolloutWorker pid=8607)\u001b[0m AGENT COORDINATES: 71 60 terrorist_0 \n",
      "\u001b[36m(RolloutWorker pid=8607)\u001b[0m \n",
      "\u001b[36m(RolloutWorker pid=8607)\u001b[0m \n",
      "\u001b[36m(RolloutWorker pid=8607)\u001b[0m \n",
      "\u001b[36m(RolloutWorker pid=8607)\u001b[0m \n",
      "\u001b[36m(RolloutWorker pid=8607)\u001b[0m \n",
      "\u001b[36m(RolloutWorker pid=8607)\u001b[0m \n",
      "\u001b[36m(RolloutWorker pid=8607)\u001b[0m AGENT COORDINATES: 36 50 soldier_0 \n",
      "\u001b[36m(RolloutWorker pid=8607)\u001b[0m \n",
      "\u001b[36m(RolloutWorker pid=8607)\u001b[0m \n",
      "\u001b[36m(RolloutWorker pid=8607)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-19 14:23:53,128\tWARNING util.py:62 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.algorithm import Algorithm\n",
    "\n",
    "def new_policy_mapping_fn(agent_id, episode, worker, **kwargs):\n",
    "    print(agent_id)\n",
    "    return  agent_id\n",
    "\n",
    "\n",
    "algo_w_2_policies = Algorithm.from_checkpoint(\n",
    "    checkpoint='/home/hemanthgaddey/RL_Spec_Ops_logs/PPO/PPO_123_a44ad_00000_0_2023-11-19_04-54-38/checkpoint_000006/',\n",
    "    policy_ids=[\"terrorist_0\", \"soldier_0\"],  # <- restore only those policy IDs here.\n",
    "    policy_mapping_fn=policy_map_fn,  # <- use this new mapping fn.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c72d98d2-b599-42bb-a87e-1de8a45bc466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Seeding only CHAOS!!!!!\n",
      "\n",
      "\n",
      "\n",
      "AGENT COORDINATES: 7 28 terrorist_0 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "AGENT COORDINATES: 28 50 soldier_0 \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "obs=env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59f7fe82-3c74-4cac-baa4-36bd06a4640e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Seeding only CHAOS!!!!!\n",
      "\n",
      "\n",
      "\n",
      "AGENT COORDINATES: 43 50 terrorist_0 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "AGENT COORDINATES: 18 28 soldier_0 \n",
      "\n",
      "\n",
      "\n",
      "{}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'terrorist_0'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m     sol_a \u001b[38;5;241m=\u001b[39m algo_w_2_policies\u001b[38;5;241m.\u001b[39mcompute_single_action(obs[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoldier_0\u001b[39m\u001b[38;5;124m'\u001b[39m], policy_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoldier_0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m----> 9\u001b[0m     terr_a \u001b[38;5;241m=\u001b[39m algo_w_2_policies\u001b[38;5;241m.\u001b[39mcompute_single_action(\u001b[43mobs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mterrorist_0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, policy_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mterrorist_0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m     sol_a \u001b[38;5;241m=\u001b[39m algo_w_2_policies\u001b[38;5;241m.\u001b[39mcompute_single_action(obs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoldier_0\u001b[39m\u001b[38;5;124m'\u001b[39m], policy_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoldier_0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m obs, rewards, _,_,_ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mterrorist_0\u001b[39m\u001b[38;5;124m\"\u001b[39m: terr_a, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoldier_0\u001b[39m\u001b[38;5;124m\"\u001b[39m: sol_a})\n",
      "\u001b[0;31mKeyError\u001b[0m: 'terrorist_0'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "env.reset()\n",
    "while True:\n",
    "    print(obs)\n",
    "    if(type(obs) == type(())):\n",
    "        terr_a = algo_w_2_policies.compute_single_action(obs[0]['terrorist_0'], policy_id=\"terrorist_0\")\n",
    "        sol_a = algo_w_2_policies.compute_single_action(obs[0]['soldier_0'], policy_id=\"soldier_0\")\n",
    "    else:\n",
    "        terr_a = algo_w_2_policies.compute_single_action(obs['terrorist_0'], policy_id=\"terrorist_0\")\n",
    "        sol_a = algo_w_2_policies.compute_single_action(obs['soldier_0'], policy_id=\"soldier_0\")\n",
    "    obs, rewards, _,_,_ = env.step({\"terrorist_0\": terr_a, \"soldier_0\": sol_a})\n",
    "    #out.clear_output(wait=True)\n",
    "    time.sleep(0.1)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4f4d16a8-ff09-4c6a-8e87-42a3d0d0fe09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Seeding only CHAOS!!!!!\n",
      "\n",
      "\n",
      "\n",
      "AGENT COORDINATES: 1 60 terrorist_0 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "AGENT COORDINATES: 45 12 soldier_0 \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "933f1886-a546-4179-9451-508a7c8a68b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Seeding only CHAOS!!!!!\n",
      "\n",
      "\n",
      "\n",
      "AGENT COORDINATES: 76 60 terrorist_0 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "AGENT COORDINATES: 11 60 soldier_0 \n",
      "\n",
      "\n",
      "\n",
      "No Seeding only CHAOS!!!!!\n",
      "\n",
      "\n",
      "\n",
      "AGENT COORDINATES: 26 28 terrorist_0 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "AGENT COORDINATES: 27 28 soldier_0 \n",
      "\n",
      "\n",
      "\n",
      "No Seeding only CHAOS!!!!!\n",
      "\n",
      "\n",
      "\n",
      "AGENT COORDINATES: 22 28 terrorist_0 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "AGENT COORDINATES: 63 60 soldier_0 \n",
      "\n",
      "\n",
      "\n",
      "Passed Parallel API test\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "import random\n",
    "from copy import copy\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import map\n",
    "import custom_fov_algo\n",
    "import gymnasium\n",
    "# from gymnasium.spaces *\n",
    "from gymnasium.spaces import Discrete, MultiDiscrete\n",
    "from pettingzoo import AECEnv\n",
    "from gymnasium.utils import EzPickle\n",
    "from pettingzoo import AECEnv, ParallelEnv\n",
    "from pettingzoo.utils import parallel_to_aec\n",
    "from pettingzoo.test import parallel_api_test\n",
    "from pettingzoo.utils import agent_selector, wrappers\n",
    "from ray.rllib import MultiAgentEnv\n",
    "from pettingzoo.utils.conversions import parallel_wrapper_fn\n",
    "from visualizer import Visualizer\n",
    "\n",
    "#Default variables\n",
    "MAP_SIZE = (80, 80)\n",
    "\n",
    "def angle_from_agent(px, py, sx, sy): # (px,py) are the coordinates from which (sx,sy) angle is measured\n",
    "    x = sx - px\n",
    "    y = sy - py\n",
    "    angle = math.atan2(y, x)\n",
    "    if angle < 0:\n",
    "        angle += 2 * math.pi\n",
    "    return 360-180*(1/math.pi)*angle\n",
    "\n",
    "def is_there(visible=None,corr_x=None,corr_y=None):\n",
    "    for i in visible:\n",
    "        if (corr_x,corr_y) in visible:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "def env(**kwargs):\n",
    "    env = Spec_Ops_Env(**kwargs)\n",
    "    if env.continuous:\n",
    "        env = wrappers.ClipOutOfBoundsWrapper(env)\n",
    "    else:\n",
    "        env = wrappers.AssertOutOfBoundsWrapper(env)\n",
    "    env = wrappers.OrderEnforcingWrapper(env)\n",
    "    return env\n",
    "\n",
    "\n",
    "\n",
    "parallel_env = parallel_wrapper_fn(env)\n",
    "\n",
    "class Spec_Ops_Env(ParallelEnv):\n",
    "    metadata={\n",
    "        \"name\":\"custom_environment_v0\",\n",
    "    }\n",
    "\n",
    "    def __init__(self, render_mode=None, config=None):\n",
    "        '''\n",
    "        we initialize the following:\n",
    "        - the starting location of terrorist\n",
    "        - the random initialization location of the dummy soldier\n",
    "        - timestamp\n",
    "        - possible agents\n",
    "\n",
    "        these attributes should not be changed after initialization\n",
    "        '''\n",
    "\n",
    "        self.config = config or {}\n",
    "\n",
    "        #Initializing\n",
    "        self.possible_agents=[\"terrorist_\"+str(i) for i in range(self.config.get('num_terr',1))]\n",
    "        self.possible_agents.extend([\"soldier_\"+str(i) for i in range(self.config.get('num_sol',1))])\n",
    "\n",
    "        self.observation_spaces=dict(zip(self.possible_agents, [MultiDiscrete([10]*6400)]*2))\n",
    "        self.action_spaces=dict(zip(self.possible_agents, [Discrete(6)]*2))\n",
    "\n",
    "        self.sol_visible=set() # currently visible coordinates\n",
    "        self.s_visible=set() # all the previous memory\n",
    "        self.terr_visible=set()  # currently visible coordinates\n",
    "        self.t_visible=set() # all the previous memory\n",
    "        self.agent_name_mapping = dict(\n",
    "            zip(self.possible_agents, list(range(1,len(self.possible_agents)+1)))\n",
    "        )\n",
    "        # print(self.agent_name_mapping)\n",
    "\n",
    "        self.timestamp=None\n",
    "        self.max_timestamp = self.config.get('max_timestamp', 420)\n",
    "\n",
    "        #initializing rendering screen\n",
    "        self.render_mode = self.config.get('render_mode', 'ansi')    #Check clashing with render_mode variable\n",
    "        self.map_size = self.config.get('map_size', MAP_SIZE)\n",
    "\n",
    "        self.viz = Visualizer(grid=self.map_size, agents=self.possible_agents)\n",
    "\n",
    "\n",
    "    # @functools.lru_cache(maxsize=None)\n",
    "    # def observation_spaces(self, agent):\n",
    "    #     # gymnasium spaces are defined and documented here: https://gymnasium.farama.org/api/spaces/\n",
    "    #     return self.observations[agent] #Change this\n",
    "\n",
    "    # @functools.lru_cache(maxsize=None)\n",
    "    # def action_space(self, agent):\n",
    "    #     return Discrete(6)\n",
    "    def get_agent_ids(self):\n",
    "        return self.possible_agents\n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"\n",
    "        Reset the environment to the starting point\n",
    "        it needs to initialize the follownig attributes:\n",
    "        - agents\n",
    "        - timestamp\n",
    "        - terrorist coordinates, angles\n",
    "        - soldier coordinates, angles\n",
    "        - observation\n",
    "        - infos\n",
    "        and must set up the environment so that render(), step(), and observe() can be called without an issue\n",
    "        \"\"\"\n",
    "        np.random.seed(seed) if seed else print('No Seeding only CHAOS!!!!!')\n",
    "\n",
    "        self.agents = self.possible_agents[:]\n",
    "        self.timestamp=0\n",
    "\n",
    "        self.state = {'map':map.read_map_file('Maps/map_0.txt')} #{\"map\": np.zeros((self.config.get('map_size', MAP_SIZE)))}\n",
    "        # in state terrorist is given 1 and soldier given as 2 when there are two agents\n",
    "        for agent in self.agents:\n",
    "            #VVIP NOTE: Handling for invalid inputs/Initialization required!\n",
    "            self.state[agent] = {}\n",
    "            x = -1\n",
    "            y = -1\n",
    "            while(self.state['map'][y][x]!=4 or x<0 or y<0):\n",
    "                x = np.random.randint(0,self.map_size[1])\n",
    "                y = np.random.randint(0,self.map_size[1])\n",
    "            print(\"\\n\\n\\nAGENT COORDINATES:\", x,y,agent,'\\n\\n\\n')\n",
    "            #self.state[agent]['x'], self.state[agent]['y'], self.state[agent]['angle'] = self.config.get(agent,{'x':x})['x'], self.config.get(agent,{'y':y})['y'], self.config.get(agent,{'angle':0})['angle']\n",
    "            self.state[agent]['x']=x #if self.state[agent]['x']<0 else self.state[agent]['x'] #Randomly place the terrorist on the grid, facing an arbitrary angle\n",
    "            self.state[agent]['y']=y #if self.state[agent]['y']<0 else self.state[agent]['y']\n",
    "            self.state[agent]['angle']=np.random.randint(0,359) #if self.state[agent]['angle']<0 else self.state[agent]['angle']\n",
    "            self.state[agent]['fov']=self.config.get(agent,{'fov': 90})['fov']  #Should be <179 becoz of math!\n",
    "            self.state[agent]['shoot_angle']=self.config.get(agent,{'shooting_angle': 15})['shooting_angle']\n",
    "            self.state[agent]['hp'] = 100\n",
    "            self.state['map'][self.state[agent]['y']][self.state[agent]['x']] = self.agent_name_mapping[agent] # updating the location oof soldier and terrorist in state map\n",
    "\n",
    "            #Error Checking\n",
    "            if(self.state[agent]['fov'] >= 180 or self.state[agent]['fov'] >= 180):\n",
    "                print(\"invalid fov angle agent ki icchav, chusko bey\")\n",
    "                exit()\n",
    "                \n",
    "        # print(self.state)\n",
    "        #time.sleep(10)\n",
    "        infos = dict({a: {} for a in self.agents})    #Just a dummy, we are not using it for now\n",
    "        self.observations = self.update_observations()\n",
    "        \n",
    "        return self.observations, infos\n",
    "\n",
    "    def step(self, actions):\n",
    "        \"\"\"\n",
    "        takes in an action for the current agent (specified by the agent_selection)\n",
    "        needs to update:\n",
    "        - coordinates of terrorist\n",
    "        - coordinates of soldier\n",
    "        - rotation of the terrorist\n",
    "        - rotation of the soldier\n",
    "        - termination condition\n",
    "        - rewards\n",
    "        - timestamp\n",
    "        - infos\n",
    "        - truncations\n",
    "        add any internl state  use by observe() or render()\n",
    "        \"\"\"\n",
    "\n",
    "        # If a user passes in actions with no agents, then just return empty observations, etc.\n",
    "        if not actions:\n",
    "            self.agents = []\n",
    "            return {}, {}, {}, {}, {}\n",
    "\n",
    "\n",
    "        # execute actions to update the state and get updated action masks\n",
    "        action_masks = self.move(actions)\n",
    "\n",
    "        # Initialize termination conditions and rewards\n",
    "        self.terminations = {a: False for a in self.agents}\n",
    "\n",
    "        rewards = {a: 0 for a in self.agents}   # rewards for all agents are placed in the rewards dictionary to be returned\n",
    "\n",
    "        # Calculate the rewards and punishments\n",
    "        rewards = self.get_rewards(rewards)\n",
    "\n",
    "        truncations = {a: False for a in self.agents}\n",
    "        if self.timestamp > self.max_timestamp:\n",
    "            rewards = {\"soldier\": 0, \"terrorist\": 0}    #IS THIS REQUIRED???\n",
    "            truncations = {\"soldier\": True, \"terrorist\": True}\n",
    "        self.timestamp += 1\n",
    "\n",
    "        # Get observations for each agent\n",
    "        self.observations = self.update_observations()\n",
    "        # Get dummy infos (not used for now)\n",
    "        infos = {a: {} for a in self.agents}\n",
    "\n",
    "        if any(self.terminations.values()) or all(truncations.values()):\n",
    "            self.agents = []\n",
    "\n",
    "        if self.render_mode != None and self.viz:\n",
    "            self.render()\n",
    "\n",
    "        return self.observations, rewards, self.terminations, truncations, infos\n",
    "\n",
    "    def move(self, actions):\n",
    "        action_masks = {}\n",
    "        for agent in actions.keys():        #NOTE: ADD WALLS AND OTHER AGENT COLLISION SUPPORT\n",
    "            action = actions[agent]\n",
    "\n",
    "            #Making it's current position free\n",
    "            self.state['map'][self.state[agent]['y']][self.state[agent]['x']] = 0\n",
    "            #Move the agent along with failsafes\n",
    "            if action == 0 and self.state[agent]['x'] > 0 and self.state['map'][self.state[agent]['y']][self.state[agent]['x']-1]==0:\n",
    "                self.state[agent]['x'] -= 1 # left\n",
    "            elif action == 1 and self.state[agent]['x'] < (self.map_size[0]-1) and self.state['map'][self.state[agent]['y']][self.state[agent]['x']+1]==0:\n",
    "                self.state[agent]['x'] += 1 # right\n",
    "            elif action == 2 and self.state[agent]['y'] > 0 and self.state['map'][self.state[agent]['y']-1][self.state[agent]['x']]==0:\n",
    "                self.state[agent]['y'] -= 1 # top\n",
    "            elif action == 3 and self.state[agent]['y'] < (self.map_size[1]-1) and self.state['map'][self.state[agent]['y']+1][self.state[agent]['x']]==0:\n",
    "                self.state[agent]['y'] += 1 # bottom\n",
    "            elif action == 4 :\n",
    "                self.state[agent]['angle'] += 30 # rotate 30 degrees anti clockwise\n",
    "                if self.state[agent]['angle']>360:\n",
    "                    self.state[agent]['angle']=self.state[agent]['angle']-360\n",
    "            elif action == 5 :\n",
    "                self.state[agent]['angle'] -= 30 # rotate 30 degrees clockwise\n",
    "                if self.state[agent]['angle']<0:\n",
    "                    self.state[agent]['angle']=self.state[agent]['angle']+360\n",
    "\n",
    "            #Marking the newly occupied position of agent in the state map\n",
    "            self.state['map'][self.state[agent]['y']][self.state[agent]['x']] = self.agent_name_mapping[agent]\n",
    "            #Generate Action masks\n",
    "            action_mask = np.ones(6, dtype=np.int8)\n",
    "            if self.state[agent]['x'] == 0 or self.state['map'][self.state[agent]['y']][self.state[agent]['x']-1]!=0:\n",
    "                action_mask[0] = 0\n",
    "            if self.state[agent]['x'] == self.map_size[1]-1 or self.state['map'][self.state[agent]['y']][self.state[agent]['x']+1]!=0:\n",
    "                action_mask[1] = 0\n",
    "            if self.state[agent]['y'] == 0 or self.state['map'][self.state[agent]['y']-1][self.state[agent]['x']]!=0:\n",
    "                action_mask[2] = 0\n",
    "            if self.state[agent]['y'] == self.map_size[0]-1 or self.state['map'][self.state[agent]['y']+1][self.state[agent]['x']]!=0:\n",
    "                action_mask[3] = 0\n",
    "\n",
    "            action_masks[agent] = action_mask\n",
    "        return action_masks\n",
    "\n",
    "\n",
    "    def get_rewards(self, rewards=None):\n",
    "        rewards = {a: 0 for a in self.agents}\n",
    "        # return rewards\n",
    "        reward_t={a: 0 for a in self.agents}\n",
    "        reward_s={a: 0 for a in self.agents}\n",
    "        #Calculate the rewards and punishments\n",
    "        for agent in self.agents:\n",
    "\n",
    "            if agent.split(\"_\")[0]==\"terrorist\":\n",
    "\n",
    "                for i in self.agents:\n",
    "                    if i.split(\"_\")[0]==\"soldier\":\n",
    "                        # print(\"soldier_corr:\",( self.state[i]['x'], self.state[i]['y']))\n",
    "                        # print(\"terr vis coor:\",self.terr_visible)\n",
    "                        # print(is_there(self.terr_visible,self.state[i]['x'],self.state[i]['y']))\n",
    "                        angle_soldier = angle_from_agent(self.state[agent]['x'], self.state[agent]['y'], self.state[i]['x'], self.state[i]['y'])\n",
    "                        # right most angles\n",
    "                        ss1 = self.state[agent]['angle']-self.state[agent]['shoot_angle']/2 #self.terr_angle-self.shoot_angle/2\n",
    "                        if(ss1<0) : ss1 = 360+ss1\n",
    "                        tt1 = self.state[agent]['angle']-self.state[agent]['fov']/2 #self.terr_angle-self.terr_fov/2\n",
    "                        if(tt1<0): tt1 = 360+tt1\n",
    "                        # left most angles\n",
    "                        ss2 = self.state[agent]['angle']+self.state[agent]['shoot_angle']/2 #self.terr_angle+self.shoot_angle/2\n",
    "                        if(ss2>=360): ss2=ss2-360\n",
    "                        tt2 = self.state[agent]['angle']+self.state[agent]['fov']/2 #self.terr_angle+self.terr_fov/2\n",
    "                        if(tt2>=360): tt2=tt2-360\n",
    "                        # print(\"terrorist pov:\",tt1,angle_soldier,tt2,self.terr_angle)\n",
    "                        '''the scope of angle is 0<=angle<=359, there is no 360'''\n",
    "                        # if(((angle_soldier >= tt1) and tt2 >= (angle_soldier)) and (tt2>tt1)):\n",
    "                        #     rewards={\"soldier\":0, \"terrorist\":1}\n",
    "                        #     terminations = {a: True for a in self.agents}\n",
    "                        # elif((tt2<tt1) and (angle_soldier<=tt2 or angle_soldier>=tt1)):\n",
    "                        #     rewards={\"soldier\":0, \"terrorist\":1}\n",
    "                        #     terminations = {a: True for a in self.agents}\n",
    "                        # else:\n",
    "                        #     rewards={\"soldier\":1, \"terrorist\":-1}\n",
    "\n",
    "                        #Terrorist: FOV & Shoot Range Rewards/Punishments\n",
    "                        if tt2>tt1 :\n",
    "                            if ((angle_soldier>=tt1) and (angle_soldier<ss1) and is_there(self.terr_visible,self.state[i]['x'],self.state[i]['y'])): # soldier in between right most shoot and fov line\n",
    "                                reward_t={\"soldier\":-1, \"terrorist\":2}\n",
    "                            elif((angle_soldier>=ss1) and (angle_soldier<=ss2) and is_there(self.terr_visible,self.state[i]['x'],self.state[i]['y'])): # soldier in the shooting angle\n",
    "                                reward_t={\"soldier\":-3, \"terrorist\":3}\n",
    "                                self.terminations = {a: True for a in self.agents}\n",
    "                            elif((angle_soldier>ss2) and (angle_soldier<=tt2) and is_there(self.terr_visible,self.state[i]['x'],self.state[i]['y'])):\n",
    "                                reward_t={\"soldier\":-1, \"terrorist\":2}\n",
    "                            else:\n",
    "                                reward_t={\"soldier\":2, \"terrorist\":-1}\n",
    "                        else:\n",
    "                            if tt1>ss1:\n",
    "                                if ((((angle_soldier>=tt1) and (angle_soldier>ss1)) or ((angle_soldier<tt1) and (angle_soldier<ss1))) and is_there(self.terr_visible,self.state[i]['x'],self.state[i]['y'])): # soldier in between right most shoot and fov line\n",
    "                                    reward_t={\"soldier\":-1, \"terrorist\":2}\n",
    "                                elif((angle_soldier>=ss1) and (angle_soldier<=ss2) and is_there(self.terr_visible,self.state[i]['x'],self.state[i]['y'])): # soldier in the shooting angle\n",
    "                                    reward_t={\"soldier\":-3, \"terrorist\":3}\n",
    "                                    self.terminations = {a: True for a in self.agents}\n",
    "                                elif((angle_soldier>ss2) and (angle_soldier<=tt2) and is_there(self.terr_visible,self.state[i]['x'],self.state[i]['y'])):\n",
    "                                    reward_t={\"soldier\":-1, \"terrorist\":2}\n",
    "                                else:\n",
    "                                    reward_t={\"soldier\":2, \"terrorist\":-1}\n",
    "                            elif ss1>ss2:\n",
    "                                if ((angle_soldier>=tt1) and (angle_soldier<ss1) and is_there(self.terr_visible,self.state[i]['x'],self.state[i]['y'])): # soldier in between right most shoot and fov line\n",
    "                                    reward_t={\"soldier\":-1, \"terrorist\":2}\n",
    "                                elif((((angle_soldier>=ss1) and (angle_soldier>ss2)) or ((angle_soldier<ss1) and (angle_soldier<=ss2))) and is_there(self.terr_visible,self.state[i]['x'],self.state[i]['y'])):\n",
    "                                    reward_t={\"soldier\":-3, \"terrorist\":3}\n",
    "                                    self.terminations = {a: True for a in self.agents}\n",
    "                                elif((angle_soldier>ss2) and (angle_soldier<=tt2) and is_there(self.terr_visible,self.state[i]['x'],self.state[i]['y'])):\n",
    "                                    reward_t={\"soldier\":-1, \"terrorist\":2}\n",
    "                                else:\n",
    "                                    reward_t={\"soldier\":2, \"terrorist\":-1}\n",
    "                            else:\n",
    "                                if ((angle_soldier>=tt1) and (angle_soldier<ss1) and is_there(self.terr_visible,self.state[i]['x'],self.state[i]['y'])): # soldier in between right most shoot and fov line\n",
    "                                    reward_t={\"soldier\":-1, \"terrorist\":2}\n",
    "                                elif((angle_soldier>=ss1) and (angle_soldier<=ss2) and is_there(self.terr_visible,self.state[i]['x'],self.state[i]['y'])): # soldier in the shooting angle\n",
    "                                    reward_t={\"soldier\":-3, \"terrorist\":3}\n",
    "                                    self.terminations = {a: True for a in self.agents}\n",
    "                                elif((((angle_soldier>tt2) and (angle_soldier>ss2)) or ((angle_soldier<=tt2) and (angle_soldier<ss2))) and is_there(self.terr_visible,self.state[i]['x'],self.state[i]['y'])):\n",
    "                                    reward_t={\"soldier\":-1, \"terrorist\":2}\n",
    "                                else:\n",
    "                                    reward_t={\"soldier\":2, \"terrorist\":-1}\n",
    "            else:\n",
    "                for i in self.agents:\n",
    "                    if i.split(\"_\")[0]==\"terrorist\":\n",
    "\n",
    "                        # #Soldier: FOV & Shoot Range Rewards/Punishments\n",
    "                        # tt1_ = tt1  #Saving variables\n",
    "                        # tt2_ = tt2\n",
    "                        # ss1_ = ss1\n",
    "                        # ss2_ = ss2\n",
    "                        # angle_soldier_ = angle_soldier\n",
    "                        # print(\"#############################################\")\n",
    "                        # print(\"terr_corr:\",(self.state[i]['x'], self.state[i]['y']))\n",
    "                        # print(\"sol vis coor:\",self.sol_visible)\n",
    "                        # print(is_there(self.sol_visible,self.state[i]['x'],self.state[i]['y']))\n",
    "                        angle_soldier = angle_from_agent(self.state[agent]['x'], self.state[agent]['y'], self.state[i]['x'], self.state[i]['y']) #angle_from_agent(self.sol_x, self.sol_y, self.terr_x, self.terr_y)\n",
    "                        # right most angles\n",
    "                        ss1 = self.state[agent]['angle']-self.state[agent]['shoot_angle']/2 #self.sol_angle-self.shoot_angle/2\n",
    "                        if(ss1<0) : ss1 = 360+ss1\n",
    "                        tt1 = self.state[agent]['angle']-self.state[agent]['fov']/2 #self.sol_angle-self.terr_fov/2\n",
    "                        if(tt1<0): tt1 = 360+tt1\n",
    "                        # left most angles\n",
    "                        ss2 = self.state[agent]['angle']+self.state[agent]['shoot_angle']/2 #self.sol_angle+self.shoot_angle/2\n",
    "                        if(ss2>=360): ss2=ss2-360\n",
    "                        tt2 = self.state[agent]['angle']+self.state[agent]['fov']/2 #self.sol_angle+self.terr_fov/2\n",
    "                        if(tt2>=360): tt2=tt2-360\n",
    "                        # print(\"soldier pov:\",tt1,angle_soldier,tt2,self.sol_angle)\n",
    "\n",
    "                        if tt2>tt1 :\n",
    "                            if ((angle_soldier>=tt1) and (angle_soldier<ss1) and is_there(self.sol_visible,self.state[i]['x'],self.state[i]['y'])): # terrorist in between right most shoot and fov line\n",
    "                                reward_s={\"soldier\":2, \"terrorist\":-1}\n",
    "                            elif((angle_soldier>=ss1) and (angle_soldier<=ss2) and is_there(self.sol_visible,self.state[i]['x'],self.state[i]['y'])): # terrorist in the shooting angle\n",
    "                                reward_s={\"soldier\":3, \"terrorist\":-3}\n",
    "                                self.terminations = {a: True for a in self.agents}\n",
    "                            elif((angle_soldier>ss2) and (angle_soldier<=tt2) and is_there(self.sol_visible,self.state[i]['x'],self.state[i]['y'])):\n",
    "                                reward_s={\"soldier\":2, \"terrorist\":-1}\n",
    "                            else:\n",
    "                                reward_s={\"soldier\":-1, \"terrorist\":2}\n",
    "                        else:\n",
    "                            if tt1>ss1:\n",
    "                                if ((((angle_soldier>=tt1) and (angle_soldier>ss1)) or ((angle_soldier<tt1) and (angle_soldier<ss1))) and is_there(self.sol_visible,self.state[i]['x'],self.state[i]['y'])): # soldier in between right most shoot and fov line\n",
    "                                    reward_s={\"soldier\":2, \"terrorist\":-1}\n",
    "                                elif((angle_soldier>=ss1) and (angle_soldier<=ss2) and is_there(self.sol_visible,self.state[i]['x'],self.state[i]['y'])): # terrorist in the shooting angle\n",
    "                                    reward_s={\"soldier\":3, \"terrorist\":-3}\n",
    "                                    self.terminations = {a: True for a in self.agents}\n",
    "                                elif((angle_soldier>ss2) and (angle_soldier<=tt2) and is_there(self.sol_visible,self.state[i]['x'],self.state[i]['y'])):\n",
    "                                    reward_s={\"soldier\":2, \"terrorist\":-1}\n",
    "                                else:\n",
    "                                    reward_s={\"soldier\":-1, \"terrorist\":2}\n",
    "                            elif ss1>ss2:\n",
    "                                if ((angle_soldier>=tt1) and (angle_soldier<ss1) and is_there(self.sol_visible,self.state[i]['x'],self.state[i]['y'])): # terrorist in between right most shoot and fov line\n",
    "                                    reward_s={\"soldier\":2, \"terrorist\":-1}\n",
    "                                elif((((angle_soldier>=ss1) and (angle_soldier>ss2)) or ((angle_soldier<ss1) and (angle_soldier<=ss2))) and is_there(self.sol_visible,self.state[i]['x'],self.state[i]['y'])):\n",
    "                                    reward_s={\"soldier\":3, \"terrorist\":-3}\n",
    "                                    self.terminations = {a: True for a in self.agents}\n",
    "                                elif((angle_soldier>ss2) and (angle_soldier<=tt2) and is_there(self.sol_visible,self.state[i]['x'],self.state[i]['y'])):\n",
    "                                    reward_s={\"soldier\":2, \"terrorist\":-1}\n",
    "                                else:\n",
    "                                    reward_s={\"soldier\":-1, \"terrorist\":2}\n",
    "                            else:\n",
    "                                if ((angle_soldier>=tt1) and (angle_soldier<ss1) and is_there(self.sol_visible,self.state[i]['x'],self.state[i]['y'])):\n",
    "                                    reward_s={\"soldier\":2, \"terrorist\":-1}\n",
    "                                elif((angle_soldier>=ss1) and (angle_soldier<=ss2) and is_there(self.sol_visible,self.state[i]['x'],self.state[i]['y'])):\n",
    "                                    reward_s={\"soldier\":3, \"terrorist\":-3}\n",
    "                                    self.terminations = {a: True for a in self.agents}\n",
    "                                elif((((angle_soldier>tt2) and (angle_soldier>ss2)) or ((angle_soldier<=tt2) and (angle_soldier<ss2))) and is_there(self.sol_visible,self.state[i]['x'],self.state[i]['y'])):\n",
    "                                    reward_s={\"soldier\":2, \"terrorist\":-1}\n",
    "                                else:\n",
    "                                    reward_s={\"soldier\":-1, \"terrorist\":2}\n",
    "        # print('sr:',reward_s)\n",
    "        # print('tr:',reward_t)\n",
    "        for i in (rewards.keys()):\n",
    "            rewards[i]=reward_s[i.split(\"_\")[0]]+reward_t[i.split(\"_\")[0]]\n",
    "        # print('rew:',rewards)\n",
    "        self.sol_visible.clear()\n",
    "        self.terr_visible.clear()\n",
    "        return rewards \n",
    "\n",
    "    def update_observations(self):\n",
    "        # New code giving entire state to both agents\n",
    "        obs={}\n",
    "        for agent in self.agents:\n",
    "            if agent.split(\"_\")[0]==\"soldier\":\n",
    "                obs_map=self.state['map'].copy()\n",
    "                obs[agent]=obs_map.flatten()\n",
    "            else:\n",
    "                obs_map=self.state['map'].copy()\n",
    "                obs[agent]=obs_map.flatten()\n",
    "        return obs\n",
    "    \n",
    "        # Old code with detailed observation fov restriction\n",
    "\n",
    "        # -1 for wall, 1 for terrorist, 2 for soldier, 0 for empty space, 3 for unknown region\n",
    "        obs={}\n",
    "        def is_blocking(x, y):\n",
    "            if((x<0) or (x>=80) or (y<0) or (y>=80)):\n",
    "                return True\n",
    "            elif((self.state['map'][y][x] != 0)):\n",
    "                return True\n",
    "            return False\n",
    "        # for agent in self.agents:\n",
    "        #     if agent.split(\"_\")[0]==\"soldier\":\n",
    "        #         self.sol_visible=set()\n",
    "        #     else:\n",
    "        #         self.terr_visible=set()\n",
    "        for agent in self.agents:\n",
    "            if agent.split(\"_\")[0]==\"soldier\":\n",
    "                # is_visible=\n",
    "                def reveal(x, y):\n",
    "                    if x>=0 and y>=0 and x<=self.map_size[1] and y<self.map_size[0]:\n",
    "                        self.sol_visible.add((x, y))\n",
    "                        self.s_visible.add((x,y))\n",
    "                # du=self.state\n",
    "                obs_map=self.state['map'].copy()\n",
    "                custom_fov_algo.compute_fov((self.state[agent]['x'],self.state[agent]['y']), self.state[agent]['angle'], self.state[agent]['fov'], is_blocking, reveal)\n",
    "                # print(is_visible)\n",
    "                for i in range(obs_map.shape[0]):\n",
    "                    for j in range(obs_map.shape[1]):\n",
    "                        # print(\"cord:\",i,j)\n",
    "                        if((j,i) in self.s_visible):\n",
    "                            pass\n",
    "                        else:\n",
    "                            obs_map[i][j]=3\n",
    "                # print(dup_state)\n",
    "                obs[agent]=obs_map.flatten()\n",
    "            else:\n",
    "                # is_visible=\n",
    "                def reveal(x, y):\n",
    "                    if x>=0 and y>=0 and x<=self.map_size[1] and y<self.map_size[0]:\n",
    "                        self.terr_visible.add((x, y))\n",
    "                        self.t_visible.add((x,y))\n",
    "                # du=self.state\n",
    "                obs_map=self.state['map'].copy()\n",
    "                custom_fov_algo.compute_fov((self.state[agent]['x'],self.state[agent]['y']), self.state[agent]['angle'], self.state[agent]['fov'], is_blocking, reveal)\n",
    "                # print(is_visible)\n",
    "                for i in range(obs_map.shape[0]):\n",
    "                    for j in range(obs_map.shape[1]):\n",
    "                        # print(\"cord:\",i,j)\n",
    "                        if((j,i) in self.t_visible):\n",
    "                            pass\n",
    "                        else:\n",
    "                            obs_map[i][j]=3\n",
    "                # print(dup_state)\n",
    "                obs[agent]=obs_map.flatten()\n",
    "        #print(\"                       NIBBBBBBBBBAAAAAA!!!!!!!!!!!!!!!!!!                          \",obs)\n",
    "        return obs\n",
    "\n",
    "    def render(self):\n",
    "        # if(self.vizz == False):\n",
    "        #     return\n",
    "        \"\"\"Renders the environment.\"\"\"\n",
    "        # CLI Rendering\n",
    "        # os.system('cls' if os.name == 'nt' else 'clear')\n",
    "        # for i in self.state['map']:\n",
    "        #     for j in i:\n",
    "        #         print(j,end='')\n",
    "        #     print()\n",
    "        # print('------------------------------------\\n\\n\\n')\n",
    "\n",
    "        self.viz.update(self.state, self.agents)\n",
    "        #time.sleep(0.0)\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        CLose releases the pygame graphical display when env is no longer being used.\n",
    "        \"\"\"\n",
    "        self.viz.quit()\n",
    "        pass\n",
    "\n",
    "from pettingzoo.test import api_test\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    env=Spec_Ops_Env()\n",
    "    # parallel_api_test(env, num_cycles=1000, verbose_progress=False)\n",
    "    parallel_api_test(env, num_cycles=1000)\n",
    "    # observations, infos = env.reset()\n",
    "\n",
    "    while env.agents:\n",
    "        #print(env.agents)\n",
    "        # this is where you would insert your policy\n",
    "        actions = {agent: env.action_space(agent).sample() for agent in env.agents}\n",
    "        #print(actions)\n",
    "        observations, rewards, terminations, truncations, infos = env.step(actions)\n",
    "        env.render()\n",
    "\n",
    "        # for i in observations['terrorist_0']:\n",
    "        # print(\"terr:\", np.unique(observations['terrorist_0']))\n",
    "        # print(\"sol:\", np.unique(observations['soldier_0']))\n",
    "        # print(\"observations:\", observations['terrorist_0'])\n",
    "        #print(\"rewards:\", rewards)\n",
    "        #print(\"----------------------------------------\")\n",
    "        # visualize_environment(env, observations)\n",
    "        # break\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bd2ff8-2433-4442-bdd0-66dea9ca24bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd81d33-b446-4d41-aabf-078ada5cbee1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
